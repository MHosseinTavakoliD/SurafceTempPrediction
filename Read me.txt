First step:
    Gathering data
    folder: CollectingData
        FirstGetStationsFromWisconsinAPI.py
            Terget: preparing a list of stations from Wisconsin API
            This file gets the inforamtion (station ID) and name of each station (commented out).
            The list of stations is there to use in the SecondGet...py file
            I wrote some notes how each station is passing the data

        SecondGetDataHourlyFromWiscAPI.py
            Target: Scrape the Wisconsin API
            This code connects to Wisconsin API based on the station_id from the previous file, capture the data
            you can change the year, month, day, hour ...
            Right now, the data captured monthly and saves in the folder "records"

Second Step:
    folder: DataCollection
        Here is basically, where we store the data from stations with good readings. So far, we just have the data from
        Wisconsin 2023 from Jan to Mid Nov. every 5 minutes

Third Step:
    folder: DataCleaning
    Target: The main target here is clear/standardize the data for ML purpose.
        WiscDataCleaning.py
            Standardizing the data based on these headings.
                    'MeasureTime',
                    'Rel. Humidity%',
                    'Air Temperature°F',
                    'Surface Temperature°F',
                    'Wind Speed (act)mph',
                    'Precipitation Intensityin/h',
                    'Saline Concentration%',
                    'Friction',
                    'Ice Percent%',
                    'Station_name',
                    'County',
                    'Latitude',
                    'Longitude'
            The standard data is saved in the CleanDB folder as a CSV file. There is a little visualization/description in the CleanDB folder

Fourth Step:
    folder:
    Target: Clustering the data for using in the ML
        Clustering: location, data continuity, removing excess data (current data is every 10 min we want it every hour)
